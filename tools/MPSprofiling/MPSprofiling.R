#####################################
##
## What: MPSprofiling.R
## Who : Frank RÃ¼hle
## When: 29-01-2020
##
## Processing barcode count data and calculation of protein stability indices (PSIs)
##
## Args:
## -----
## targets=targets.txt      # file describing the targets.
## prefix=RE                # prefix to remove from the sample name
## suffix=RE                # suffix to remove from the sample name (usually _readcounts.tsv)
## inputdir=.               # input directory where the files .tsv files are located
## out="MPSprofiling"       # output directory
## expdesign="amplicon1     # experiment design
## threshold_rel_countssum=0        # threshold for first qc filtering (proportion of sample sum from mean sample sum)
## excludeSeqsNotInAllFractions =F  # discard all sequences per sub_experiment, which were not detected in all fractions of this sub_eperiment
######################################

options(stringsAsFactors=FALSE)
library(parallel)
library(reshape2)
library(kableExtra)
#library(plyr)
library(dplyr)
library(tidyr)
library(Biobase)
library(Biostrings)
library(mixtools)
library(RColorBrewer)
library(gplots)
library(ggplot2)
library(ggbeeswarm)
library(ggrepel)
library(pheatmap)

##
## get arguments from the command line
##
parseArgs <- function(args,string,default=NULL,convert="as.character") {

    if(length(i <- grep(string,args,fixed=T)) == 1)
        return(do.call(convert,list(gsub(string,"",args[i]))))
    
    if(!is.null(default)) default else do.call(convert,list(NA))
}

args <- commandArgs(T)
ftargets     <- parseArgs(args,"targets=","targets.txt")     # file describing the targets
pre          <- parseArgs(args,"prefix=","")    # prefix to remove from the sample name
suf          <- parseArgs(args,"suffix=","")    # suffix to remove from the sample name
inputdir     <- parseArgs(args,"inputdir=","./")     # input files directory
out          <- parseArgs(args,"out=","MPSprofiling") # output directory
expdesign  <- parseArgs(args,"expdesign=","amplicon1")  
threshold_rel_countssum  <- parseArgs(args,"threshold_rel_countssum=", 0, convert="as.numeric")  # threshold for first qc filtering (proportion of sample sum from mean sample sum)
removeLowCountsRaw <- parseArgs(args,"removeLowCountsRaw=", FALSE, convert="as.logical")  
minCountThreshold  <- parseArgs(args,"minCountThreshold=", 0, convert="as.numeric")
excludeSeqsNotInAllFractions  <- parseArgs(args,"excludeSeqsNotInAllFractions=", FALSE, convert="as.logical")  # discard all sequences per sub_experiment, which were not detected in all fractions of this sub_experiment
remove_NA_bynuc_PSI  <- parseArgs(args,"remove_NA_bynuc_PSI=", TRUE, convert="as.logical")  
remove_NA_byaa_pooledPSI  <- parseArgs(args,"remove_NA_byaa_pooledPSI=", TRUE, convert="as.logical")  
invertedDesign  <- parseArgs(args,"invertedDesign=", FALSE, convert="as.logical")  





runstr <- "Rscript MPSprofiling.R [targets=targets.txt] [prefix=RE] [suffix=RE] [inputdir=.] [out=MPSprofiling] [expdesign=amplicon1] [threshold_rel_countssum=0] [excludeSeqsNotInAllFractions=F] [remove_NA_bynuc_PSI=T] [remove_NA_byaa_pooledPSI=T] [invertedDesign=F]"
if(!file.exists(ftargets))   stop(paste("File",ftargets,"does NOT exist. Run with:\n",runstr))
if(!file.exists(inputdir))   stop(paste("Dir",inputdir,"does NOT exist. Run with:\n",runstr))


### columns in target file
# experiment: overall experiment name.
# sub_experiment: summarizes those samples which have been distributed to stability bins and belong together for PSI calculation.
# sample: sample identifier. If no demultiplexing necessary, may be same as file.
# file: input file names after removing common prefixes and suffixes. It is grepped against count file name to merge targets.txt to the count data.
# fraction: fraction of cells assiged to each bin.
# bin: number of signal intesity bin.
# barcode_demultiplex (if design is "amplicon3") for demultiplexing samples from count file.


# parameters
required_target_columns <- c("file", "sample", "experiment", "sub_experiment", "bin", "fraction")
need2demultiplex <- expdesign %in% c("amplicon3")
if(need2demultiplex) {required_target_columns <- c(required_target_columns, "barcode_demultiplex")}
annoFactors <- c("experiment", "sub_experiment")
applyThreshBeforeBGsubt =FALSE # for background subtraction: apply cutoff threshold before subtracting background value (not done in CombinatorialProfiler)
removeZerosAfterBGsubt = FALSE # remove zero counts generated by background subtraction (not done in CombinatorialProfiler)   


# create output folders
if(!dir.exists(out)) {dir.create(out, recursive = T)}
name_plotfolder <- file.path(out, "plots")
if(!dir.exists(name_plotfolder)) {dir.create(name_plotfolder, recursive = T)}


# read targets file
targets <- read.delim(ftargets, header=T, comment.char="#", sep="\t")
if (!all(required_target_columns %in% colnames(targets))) {
  missing_cols <- paste(required_target_columns[! required_target_columns %in% colnames(targets)], collapse=", ")
  stop("\n\nMissing required target column(s):", missing_cols)
}
targets <- targets[,required_target_columns]

# read count files
  f <- list.files(paste0(inputdir), pattern="\\.barcode_count\\.tsv$", full.names=TRUE) 
  f <- f[!grepl("Undetermined", f)] # don't use count file with unmapped reads if present
  
  counts <- mclapply(f, read.delim, header=T, comment.char = "#")
  names(counts) <- basename(f)
  
  # merge counts with targets file (includes optional demultiplexing)
  counts <- lapply(names(counts), function(x) {
    counts[[x]] <- counts[[x]][,1:3]
    colnames(counts[[x]]) <- c("sequence", "id", "count")
    
    if(need2demultiplex) { 
        counts[[x]] <- counts[[x]][counts[[x]]$sequence != "", ] # remove entries when variable region is empty string
        # if true, counts[[x] contains multiple samples in one file instead of just 1 sample. 
        # id is 2nd extracted barcode and must correspond to "barcode_demultiplex" in targets.txt.
        logindex_targetsfile <- sapply(targets$file, grepl, x)
        if(sum(logindex_targetsfile) != length(unique(counts[[x]]$id))) {
          stop("\nfile in targets.txt does not correspond to the correct number of ids in count file ", x)}
        merge(counts[[x]], targets[logindex_targetsfile,], all.x=T, by.x="id", by.y="barcode_demultiplex")
    } else { 
        # if false, counts[[x]] contains a single sample. 
        # id is sample file name including bpipe suffixes (not needed since count file name is used)
        logindex_targetsfile <- sapply(targets$file, grepl, x)
        if(sum(logindex_targetsfile) !=1) {stop("\nfile in targets.txt not unique for count file name ", x)}
        counts[[x]] <- data.frame(counts[[x]][, c("sequence", "count")], targets[logindex_targetsfile,])
    }
  })

  counts <- bind_rows(counts) # rbind list elements to data.frame
  
  # remove barcodes containg Ns
  counts <- counts[!grepl("N", counts$sequence), ]
    

  # sum all counts per sample for initial QC filtering
  counts_summary <- counts %>%
    dplyr::group_by(sample) %>%
    dplyr::summarize(sum = sum(count)) %>%
    dplyr::ungroup() 
    counts <- merge(counts, counts_summary, by="sample", all.x=T)
      
      # 1st filter step for extreme outlier samples (need counts_summary for correct mean(counts_summary$sum))
      removedSamples <- as.data.frame(counts_summary[counts_summary$sum < threshold_rel_countssum * mean(counts_summary$sum), c("sample", "sum")])
      cat("\nremove", nrow(removedSamples), "samples from dataset with <", 100*threshold_rel_countssum, "% of mean total counts:\n")
      print(removedSamples)
      counts <- filter(counts, !(sum < threshold_rel_countssum * mean(counts_summary$sum)))
      
      
  ## rev comp sequence in case of inverted design
  if(invertedDesign) {
    counts$sequence <- factor(as.character(Biostrings::reverseComplement(Biostrings::DNAStringSet(as.character(counts$sequence)))))    
    }
      
  
    #### start analysis 
        
      # factorize categorical variables after subsetting and initial outlier removal
      category_vars <- colnames(counts)[colnames(counts) %in% c("sample", "experiment", "sub_experiment", "bin", "sequence")]
      counts[,category_vars] <- lapply(counts[,category_vars], factor)
      
      # define helper catergory
      counts$helper_cat <- factor(paste(counts$experiment, counts$sub_experiment, sep="_"))
      
      # add columns bin_rank per sub_experiment and column totalfractions. Collapse dataset with unique()
      counts_summary2 <- unique(counts[,colnames(counts) %in% c("sample", "experiment", "sub_experiment", "bin", "helper_cat")]) %>%
      dplyr::group_by(helper_cat)  %>%
      dplyr::mutate(bin_rank=rank(bin)) %>% 
      dplyr::mutate(totalfractions = dplyr::n_distinct(bin)) %>% # number of fractions per sample
      dplyr::ungroup() 

    counts <- merge(counts, counts_summary2[,c("sample", "bin_rank", "totalfractions")], by="sample", all.x=T)
    
    # add column f as transformed bin index [0,1]
    counts$f <- (counts$bin_rank -1) / (counts$totalfractions -1) 
  
  
  # in case there are sequences with zero counts, these entries can be removed (there are no zero counts since only observed sequences are counted)
  if(removeLowCountsRaw) {
    logindex_nocounts <- counts$count < minCountThreshold
    if(sum(logindex_nocounts)>0) {
      cat("\n", sum(logindex_nocounts), "entries removed from raw count matrix due to counts <", minCountThreshold, "\n")
      print(counts[logindex_nocounts, c("sample", "bin", "sequence")][1:min(20,sum(logindex_nocounts)),])
      cat("\n")
      counts <- counts[!logindex_nocounts,]
    }
  }
  

  ### background subtraction
  # and run downstream calculations separated for with and without background subtraction
  
  # To try to remove the noise, we will try to fit a Gaussian mixture model to each sample and subtract the mean of the background distribution. In addition,
  # we will threshold the raw counts to remove the background distribution. There are several cases to deal with: The case of well-separable distributions,
  # like wild type fraction E, the case of non-separable distributions like ubr1-mak3 fraction H, and the case where the background distribution completely
  # dominates, like ubr1-ufd4 fraction B. The threshold depends on the shape and position of the distributions. If the 97.5th percentile of the background
  # distribution is larger than the 2.5th percentile of the foreground distribution and the mean of the background distribution is lower than the mean of
  # the foreground distribution, we set the threshold at the 2.5th percentile of the foreground distribution to include all foreground reads, since this case
  # corresponds to non-separable distribution. Otherwise we threshold at the 97.5th percentile of the background distribution, which applies to well-separable
  # cases and cases with completely dominant background distribution.
  # options(try.outFile = stdout()) 
  
  set.seed(100)
  
  bgsubt <- try(counts %>% 
    group_by(helper_cat, bin) %>% # determine bg and threshold per bin
    do({
      ret <- .
      mix <- normalmixEM(log10(ret$count), maxrestarts=500)
      ubounds <- 10^qnorm(0.975, mix$mu, mix$sigma)
      lbounds <- 10^qnorm(0.025, mix$mu, mix$sigma)
      bg_comp <- which.min(ubounds)
      if (ubounds[bg_comp] > lbounds[-bg_comp] && mix$mu[bg_comp] < mix$mu[-bg_comp]) {
        thresh <- lbounds[-bg_comp]
      } #else if (ubounds[bg_comp] > lbounds[-bg_comp] && mix$mu[bg_comp] < mix$mu[-bg_comp])
      else {
        thresh <- ubounds[bg_comp]
      }
      cat(paste("experiment:", unique(ret$helper_cat), "; bin:", unique(ret$bin), if(applyThreshBeforeBGsubt) {paste("; threshold applied:", round(thresh, 3))} else {""}, "; bg subtracted:", round(10^mix$mu[bg_comp], 3), "\n")) 
      if(applyThreshBeforeBGsubt) {ret$count <- ifelse(ret$count > thresh, ret$count, 0)} # not applied in CombinatorialProfiler
      ret$count_bgsubt <- pmax(0, ret$count - 10^mix$mu[bg_comp])
      ret$count <- ret$count_bgsubt
      ret$count_bgsubt <- NULL
      ret
    }) )

   
   if ("try-error" %in% class(bgsubt)) {
     cat("\nFailed to distinguish background by fitting a Gaussian mixture model. Background subtraction omitted!\n\n")
     bgloop <- c("raw")
   } else {
     bgloop <- c("raw", "bgsubt")
      # in case there are sequences with zero counts after backgound subtraction, these entries can be removed
      # (these entries may have emerged when background is subtracted from small count numbers)
      # Removing zero counts is not done in CombinatorialProfiler
      if(removeZerosAfterBGsubt) {
         logindex_nocounts_bgsubt <- bgsubt$count==0
         if(sum(logindex_nocounts_bgsubt)>0) {
           cat("\n", sum(logindex_nocounts_bgsubt), "entries removed from count matrix due to zero counts after background subtraction:\n")
           print(bgsubt[logindex_nocounts_bgsubt, c("sample", "bin", "sequence")][1:min(20,sum(logindex_nocounts_bgsubt)),]) # print max 20 entries
           cat("\n")
           bgsubt <- bgsubt[!logindex_nocounts_bgsubt,]
         }}
     
     # recalulate sample sum for bgsubt 
     bgsubt <- bgsubt %>%
       dplyr::group_by(sample) %>%
       dplyr::mutate(sum = sum(count)) %>%
       dplyr::ungroup() 
    }
   
   
  # initalise result lists
  counts_all <- list()
  bynuc_all <- list()
  byaa_all <- list()
  
  ### run the following code separately for counts and bgsubt
  # counts and bgsubt have identical format and differ just in column "count"
  
  for(bg in bgloop) {
  
    if(bg=="bgsubt") {counts <- bgsubt} 
   
        for (e in unique(counts$experiment)) { # 1 plot per experiment
            plot_histo <- counts[counts$experiment == e,] %>%
              filter(count > 0) %>%
              ggplot(aes(count)) +
              geom_histogram(bins=100) +
              facet_grid(helper_cat ~ bin) +
              scale_x_log10() +
              theme(axis.text.x=element_text(angle=45, vjust=1, hjust=1), strip.text.y = element_text(angle=0))
        
            ggsave(filename=file.path(name_plotfolder, paste0("histogram_",e , "_", bg, ".png")),
                   plot=plot_histo, 
                   width = 200, height = 150, 
                   units = c("mm"),  dpi = 600, device="png")
            }
        
      ## cell distribution per variant normalised by bin countsum and cell fraction:
      # divide raw counts by sum counts for all sequences from one sub_experiment and bin
      # and multiply by cell fraction
      # This normalized count (aka Cp) is used by CombinatorialProfiler
       counts$normalized_counts <- (counts$count / counts$sum) * counts$fraction
        
      # transformed bin index times normalised counts
       counts$fxCp <- counts$f * counts$normalized_counts 
      
       # define number of bins (nfractions) with non-zero counts for each sequence entry
       counts <- counts %>%
         dplyr::group_by(sequence, helper_cat) %>%
         dplyr::mutate(nfractions = sum(count>0, na.rm=T)) %>%
         dplyr::ungroup() 

      # translate nucleotide to codon with Biostrings # confimed with CombinatorialProfiler
      # no.init.codon=T because otherwise the alternative initiation codons CTG and TTG are 
      # translated to M instead to L when they are located at the first position of the hexamer.
      # alternative from GENETIC_CODE_TABLE: getGeneticCode("Alternative Yeast Nuclear", full.search=T)
      # The code "Alternative Yeast Nuclear" differs from GENETIC_CODE only in alt_init_codons
        counts$translation <- as.character(translate(DNAStringSet(as.character(counts$sequence)),
                                              genetic.code=GENETIC_CODE, no.init.codon=T))
      
      # produce output counts table
        counts <- data.frame(counts[,c(colnames(targets)[colnames(targets) %in% colnames(counts)], 
                                       "bin_rank", "f", "fxCp", "sequence", "sum", "nfractions", 
                                       "totalfractions", "translation", "count", "normalized_counts")])
        counts <- counts[order(counts$experiment, counts$sub_experiment, counts$f, counts$sequence), ]
          
 

      #######     
      ## calculate PSI (aka dsi) per hexamer (bynuc)

        bynuc <- counts %>%
          dplyr::group_by(sequence, experiment, sub_experiment, .drop=T) %>%
          dplyr::summarize(fxCp_sum= sum(fxCp, na.rm=T), # calculate PSI numerator "fxCp_sum"
                           Cp_sum= sum(normalized_counts, na.rm=T), # calculate PSI denominator "Cp_sum"
                           nfractions= sum(normalized_counts >0, na.rm=T), # non-zero fractions per sequence
                           totalfractions= max(totalfractions, na.rm=T)) %>% # total fractions per sequence (counts$totalfractions of each subset all identical)
          dplyr::ungroup() 
        bynuc$PSI <- bynuc$fxCp_sum / bynuc$Cp_sum # PSI
        bynuc$translation <- as.character(translate(DNAStringSet(as.character(bynuc$sequence)), genetic.code=GENETIC_CODE, no.init.codon=T))
        
        bynucstats <- counts %>%
          dplyr::group_by(sequence, experiment, sub_experiment, .drop=T) %>%
          dplyr::summarize_at(dplyr::vars("normalized_counts", "count"), dplyr::funs(min, max, sum, median, mean, sd), na.rm=T) %>%
          dplyr::ungroup() 
        
        bynuc <- left_join(bynuc, bynucstats, by=c("sequence", "experiment", "sub_experiment"))   
        
        # remove NA entries and sort
        # don't remove NAs from bynuc and byaa tables if PSIs cannot be calculated anyway due to lack of different bins in targets.txt
        if(all(tapply(targets$bin,targets$sub_experiment, function(x) length(unique(x))==1))) {
          remove_NA_bynuc_PSI <- FALSE
          remove_NA_byaa_pooledPSI <- FALSE
        }
        
        if(remove_NA_bynuc_PSI) {bynuc <- bynuc[!is.na(bynuc$PSI),]}
        
        bynuc <- bynuc %>%
          dplyr::arrange(experiment, sub_experiment, sequence) %>%
          dplyr::select(experiment, sub_experiment, sequence, translation, PSI, nfractions, totalfractions,  contains("count"))
        
        

          ### optional: exclude PSIs of all sequences which were not detected in all fractions of a sub_experiment
          if(excludeSeqsNotInAllFractions) {
            cat("\nDiscard all sequences per sub_experiment, which were not detected in all fractions of this sub_eperiment\n")
            counts <- counts[!(is.na(counts$nfractions) | (counts$nfractions != counts$totalfractions)), ]
            bynuc <- bynuc[!(is.na(bynuc$nfractions) | (bynuc$nfractions != bynuc$totalfractions)), ]
          }
          

          ##########
          # calculate PSI per di-residue (byaa) from bynuc
          byaaN <- bynuc %>%
            dplyr::group_by(translation, experiment, sub_experiment, .drop=T) %>%
            dplyr::summarize(median_PSI = median(PSI, na.rm=T),
                             nsequences=sum(!is.na(PSI))) %>% # number of seqs which contribute a PSI to aa
          dplyr::ungroup() 

          ## calculate pooled PSI per di-residue from counts
          byaaC <- counts %>%
            dplyr::group_by(translation, experiment, sub_experiment, .drop=T) %>%  
            dplyr::summarize(pooled_fxCp_sum = sum(fxCp),
                             pooled_Cp_sum = sum(normalized_counts),
                             nfractions =  dplyr::n_distinct(factor(ifelse(normalized_counts>0, bin, NA)), na.rm=T), # number of different non-zero fractions for seqs per aa
                             totalfractions = max(totalfractions, na.rm=T)) %>% # total fractions per sub_experiment
            dplyr::ungroup() 
            byaaC$pooled_PSI <- byaaC$pooled_fxCp_sum / byaaC$pooled_Cp_sum

          byaaCstats <- counts %>%
              dplyr::group_by(translation, experiment, sub_experiment, .drop=T) %>%
              dplyr::summarize_at(dplyr::vars("normalized_counts", "count"), dplyr::funs(min, max, sum, median, mean, sd), na.rm=T) %>%
              dplyr::ungroup() 
            
          byaa <- left_join(byaaC, byaaCstats, by=c("translation", "experiment", "sub_experiment"))   
          byaa <- left_join(byaaN, byaa, by=c("translation", "experiment", "sub_experiment"))
          
          # remove NA entries and sort
          if(remove_NA_byaa_pooledPSI) {byaa <- byaa[!is.na(byaa$pooled_PSI),]}
          
          byaa <- byaa %>%
            dplyr::arrange(experiment, sub_experiment, translation) %>%
            dplyr::select(experiment, sub_experiment, translation, median_PSI, pooled_PSI, 
                          nsequences, nfractions, totalfractions, contains("count"))
          

          ### store results in list
          counts_all[[bg]] <- counts
          bynuc_all[[bg]] <- bynuc
          byaa_all[[bg]] <- byaa
          
          write.table(counts, file= file.path(out, paste0(bg, "_counts.txt")), sep="\t", row.names = F, quote = F)
          write.table(bynuc, file= file.path(out, paste0(bg, "_PSI_bynuc.txt")), sep="\t", row.names = F, quote = F)
          write.table(byaa, file= file.path(out, paste0(bg, "_PSI_byaa.txt")), sep="\t", row.names = F, quote = F)
          
} # end bg loop      

  for (e in unique(counts$experiment)) {  # write output tables per experiment
    for(bg in names(counts_all)) {
      
      if (length(unique(counts_all[[bg]]$experiment))>1) {
        write.table(counts_all[[bg]][counts_all[[bg]]$experiment==e,], file= file.path(out, paste0(e, "_", bg, "_counts.txt")), sep="\t", row.names = F, quote = F)
        write.table(bynuc_all[[bg]][bynuc_all[[bg]]$experiment==e,], file= file.path(out, paste0(e, "_", bg, "_PSI_bynuc.txt")), sep="\t", row.names = F, quote = F)
        write.table(byaa_all[[bg]][byaa_all[[bg]]$experiment==e,], file= file.path(out, paste0(e, "_", bg, "_PSI_byaa.txt")), sep="\t", row.names = F, quote = F)
      }
    }
  }
############################################################################



#save the sessionInformation
writeLines(capture.output(sessionInfo()),paste(out, "/MPSprofiling_session_info.txt", sep=""))
save(counts_all, bynuc_all, byaa_all, file=paste0(out,"/MPSprofiling.RData"))











